{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN on faces - working!!!.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoliamShely/Face-generation-with-GAN/blob/master/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mQj2w3qBW0YN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import zipfile\n",
        "import os\n",
        "from scipy import misc\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from time import time\n",
        "\n",
        "from IPython.core.debugger import Tracer\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Conv2D, MaxPool2D, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from keras.layers.convolutional import UpSampling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import initializers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bV1WRuBnB0JN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tUCkAzLMQdMK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Enter you kaggle api info here."
      ]
    },
    {
      "metadata": {
        "id": "wh3-e1hLB1_H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!echo '{\"username\":\"your_username\",\"key\":\"your_key\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BwNs-SbiB4cT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d jessicali9530/celeba-dataset -f img_align_celeba.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JnqVgCtxB7NO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls /content\n",
        "!mkdir images "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SInpqvFizBc4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wx5ohn94bc1T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helpers"
      ]
    },
    {
      "metadata": {
        "id": "PF1xH64IB_U4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rgb2gray(rgb):\n",
        "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
        "\n",
        "def resize(img):\n",
        "  return cv2.resize(img, dsize=(64,78))#, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "def extract_images():\n",
        "  print(\"Extracting...\")\n",
        "  f = zipfile.ZipFile(\"/content/img_align_celeba.zip\", 'r')\n",
        "  f.extractall(\"images\")\n",
        "  f.close()\n",
        "  del(f)\n",
        "  print(\"Extracted.\")\n",
        "  ! mv images/img_align_celeba ./images_extracted\n",
        "  print(\"Folder moved to './images_extracted'.\")\n",
        "\n",
        "def load_images(num_of_images=50000, grayscale=False):\n",
        "  print(\"Loading images...\")\n",
        "  ret = []\n",
        "  ma = num_of_images\n",
        "  for image in os.listdir(\"images_extracted\"):\n",
        "    if(ma <= 0):\n",
        "      return np.array(ret)\n",
        "    c_image = \"images_extracted/{}\".format(image)\n",
        "    color_img = misc.imread(c_image)\n",
        "    if grayscale:\n",
        "      color_img = rgb2gray(color_img)\n",
        "    color_img = resize(color_img)\n",
        "    ret.append(color_img)\n",
        "    del(c_image)\n",
        "    ma -= 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kCMb-SsqCAPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "extract_images()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9KYvy0_6m4sA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### showImages"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Fy_YSiua6R3C",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def showImages(epoch, generator, rgb=False):\n",
        "  noise = np.random.normal(0,1, size=[10, noise_dim])\n",
        "  images = generator.predict(noise)\n",
        "  images = images.reshape(images.shape[0], height, width, channels)\n",
        "  \n",
        "  plt.figure(figsize=(10,3))\n",
        "  \n",
        "  for indx in range(images.shape[0]):\n",
        "    plt.subplot(1,10, indx+1)\n",
        "    if rgb:\n",
        "      plt.imshow(denormalize_rgb(images[indx]),interpolation='nearest')\n",
        "    else:\n",
        "      plt.imshow(images[indx].reshape(height,width), cmap='gray',interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "  plt.show()    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HXST10XHbpVZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load Images"
      ]
    },
    {
      "metadata": {
        "id": "9JhVMEtRCFhh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = load_images(num_of_images=30000) # takes 10 mins to load\n",
        "\n",
        "# The dimension of our random noise vector.\n",
        "noise_dim = 100\n",
        "width = data.shape[2]\n",
        "height = data.shape[1]\n",
        "channels = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gwSRp18NbMed",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def denormalize_rgb(m):\n",
        "  return (m*127+127).astype('u1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aKRPaWZFgtDo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AutoObj(object):\n",
        "  def __init__(self, **kwargs):\n",
        "    self.__dict__.update(**kwargs)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sNgeEhnaZ2j5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### getX_1D, 2D"
      ]
    },
    {
      "metadata": {
        "id": "nrT6m4vBprT_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getX_2D():\n",
        "  return (data.astype(np.float32)-127.5)/127.5\n",
        "\n",
        "def getX_1D():\n",
        "  ret = getX_2D()\n",
        "  ret = ret.reshape(ret.shape[0], -1)    \n",
        "  return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YDga9TMnq3mb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gen_optimizer():\n",
        "  return Adam(lr=0.0002, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zq4YkPbLaC9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### gen_generator_1D"
      ]
    },
    {
      "metadata": {
        "id": "EOt6HpGEq-CZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gen_generator_1D_working(optimizer):\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Dense(256, input_dim=noise_dim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "  model.add(BatchNormalization(momentum=0.9))\n",
        "  model.add(LeakyReLU(0.2)) # like alpha=0.2\n",
        "  \n",
        "  model.add(Dense(512))\n",
        "  model.add(BatchNormalization(momentum=0.9))\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  \n",
        "  model.add(Dense(1024))\n",
        "  model.add(BatchNormalization(momentum=0.9))\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  \n",
        "  model.add(Dense(height*width*channels, activation='tanh'))\n",
        "#   model.add(Reshape((height,width,channels))) # remove if not\n",
        "#   model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "  \n",
        "  return model\n",
        "  \n",
        "def gen_generator_1D(optimizer):\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Dense(256, input_dim=noise_dim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "  model.add(BatchNormalization(momentum=0.9))\n",
        "  model.add(LeakyReLU(0.2)) # like alpha=0.2\n",
        "  \n",
        "  model.add(Dense(height*width*channels, activation='tanh'))\n",
        "  \n",
        "  return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O1XLCsaOaKls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### gen_generator_2D"
      ]
    },
    {
      "metadata": {
        "id": "zJ5cOh7znjql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gen_generator_2D(optimizer):\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Dense(256, input_dim=noise_dim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "  model.add(BatchNormalization(momentum=0.9))\n",
        "  model.add(LeakyReLU(0.2)) \n",
        "  \n",
        "  model.add(Dense(512))\n",
        "  model.add(BatchNormalization(momentum=0.9))\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  \n",
        "  model.add(Dense(1024))\n",
        "  model.add(BatchNormalization(momentum=0.9))\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  \n",
        "  model.add(Dense(height*width*channels, activation='tanh'))\n",
        "  model.add(Reshape((height,width,channels))) # remove if not\n",
        "  \n",
        "  return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vxFg14t7aRk0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### gen_discriminator_1D"
      ]
    },
    {
      "metadata": {
        "id": "O3n5aVWHsWRM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gen_discriminator_1D(optimizer):\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Dense(1024, input_dim=height*width*channels, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  model.add(Dropout(0.3))\n",
        "  \n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  model.add(Dropout(0.3))\n",
        "  \n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  model.add(Dropout(0.3))\n",
        "  \n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "def gen_discriminator_1D_conv(optimizer):\n",
        "  model = Sequential()\n",
        "  model.add(Reshape((height, width, channels)))\n",
        "\n",
        "  model.add(Conv2D(\n",
        "      filters=32,\n",
        "      kernel_initializer=initializers.RandomNormal(stddev=0.02), \n",
        "      kernel_size=(5,5), \n",
        "      padding='SAME', \n",
        "      input_shape=(height,width,channels), \n",
        "      activation=LeakyReLU(0.2),\n",
        "  ))\n",
        "  model.add(MaxPool2D(strides=2))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(\n",
        "      filters=64, \n",
        "      kernel_size=(5,5), \n",
        "      padding='SAME',  \n",
        "      activation=LeakyReLU(0.2)\n",
        "  ))\n",
        "  model.add(MaxPool2D(strides=2))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(\n",
        "      filters=128,\n",
        "      kernel_size=(5,5), \n",
        "      padding='SAME',  \n",
        "      activation=LeakyReLU(0.2)\n",
        "  ))\n",
        "  model.add(MaxPool2D(strides=2))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation=LeakyReLU(0.2)))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gh5WraigaUzl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### gen_discriminator_2D"
      ]
    },
    {
      "metadata": {
        "id": "Gr3YiFYQVBik",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gen_discriminator_2D_conv(optimizer):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(\n",
        "      filters=32,\n",
        "      kernel_initializer=initializers.RandomNormal(stddev=0.02), \n",
        "      kernel_size=(5,5), \n",
        "      padding='SAME', \n",
        "      input_shape=(height,width,channels), \n",
        "      activation=LeakyReLU(0.2),\n",
        "  ))\n",
        "  model.add(MaxPool2D(strides=2))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(\n",
        "      filters=64,\n",
        "      kernel_size=(5,5), \n",
        "      padding='SAME',  \n",
        "      activation=LeakyReLU(0.2)\n",
        "  ))\n",
        "  model.add(MaxPool2D(strides=2))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(\n",
        "      filters=128,\n",
        "      kernel_size=(5,5), \n",
        "      padding='SAME',  \n",
        "      activation=LeakyReLU(0.2)\n",
        "  ))\n",
        "  model.add(MaxPool2D(strides=2))\n",
        "  model.add(Dropout(0.3))\n",
        "  # 8x5x128\n",
        "  model.add(Dense(128, activation=LeakyReLU(0.2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def gen_discriminator_2D(optimizer):\n",
        "  model = Sequential()\n",
        "  \n",
        "  # 64x78x3\n",
        "  model.add(Conv2D(\n",
        "      filters=64,\n",
        "      kernel_initializer=initializers.RandomNormal(stddev=0.02), \n",
        "      kernel_size=(3,3), \n",
        "      padding='SAME', \n",
        "      input_shape=(height,width,channels), \n",
        "      activation='relu'\n",
        "  ))\n",
        "#   model.add(MaxPool2D(strides=2))\n",
        "#   model.add(Dropout(0.3))\n",
        "  model.add(Conv2D(\n",
        "      filters=32,#128, \n",
        "      kernel_size=(3,3), \n",
        "      padding='SAME',  \n",
        "      activation='relu',#LeakyReLU(0.2)\n",
        "  ))\n",
        "#   model.add(MaxPool2D(strides=2))\n",
        "#   model.add(Dropout(0.3))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rTZwqqTWacRM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### gen_gan_net"
      ]
    },
    {
      "metadata": {
        "id": "9bj8QjaGtVtK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gen_gan_net(discriminator, noise_dims, generator, optimizer):\n",
        "  discriminator.trainable = False\n",
        "  gan_input = Input(shape=(noise_dims,))\n",
        "  \n",
        "  X = generator(gan_input)\n",
        "\n",
        "  gan_output = discriminator(X)\n",
        "  gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "  gan.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
        "  \n",
        "  return gan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-NlT_4RlbA2E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### train_1D"
      ]
    },
    {
      "metadata": {
        "id": "Fq2Pi0yit7rF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_1D(epochs, batch_size, show_interval,  generator,   discriminator):\n",
        "  \n",
        "  X_train = getX_1D()\n",
        "  \n",
        "  batch_count = X_train.shape[0] / batch_size\n",
        "  \n",
        "  gan = gen_gan_net(discriminator, noise_dim, generator, adam)\n",
        "  \n",
        "  for epoch in range(1, epochs+1):\n",
        "    print(\"=\"*30 + \"Epoch \" + str(epoch) + \"=\"*30)\n",
        "    d_loss = g_loss = 0\n",
        "    \n",
        "    for indx in range(int(batch_count)):\n",
        "      noise = np.random.normal(0,1, size=[batch_size, noise_dim])\n",
        "      real_images = X_train[np.random.randint(0,X_train.shape[0], size=batch_size)] # get random images from X_train. len is batch_size\n",
        "      \n",
        "      fake_images = generator.predict(noise)\n",
        "      \n",
        "#       print(\"Fake shape: {}\\tReal shape:{}\".format(fake_images.shape,real_images.shape))\n",
        "      X = np.concatenate([real_images, fake_images])\n",
        "      \n",
        "      y_dis = np.concatenate([np.ones(batch_size), np.zeros(batch_size)])\n",
        "      \n",
        "      discriminator.trainable = True\n",
        "      d_loss += discriminator.train_on_batch(X, y_dis)[0]\n",
        "      \n",
        "      \n",
        "      \n",
        "#       # train the generator\n",
        "      noise = np.random.normal(0,1, size=[batch_size, noise_dim])\n",
        "      y_gen = np.ones(batch_size)\n",
        "#       print(\"noise shape: {}\\ty_gen shape: {}\".format(noise.shape, y_gen.shape))\n",
        "      discriminator.trainable = False\n",
        "      g_loss += gan.train_on_batch(noise, y_gen)[0]\n",
        "\n",
        "    print(\"g_loss={}\\td_loss={}\".format(g_loss/batch_count, d_loss/batch_count))\n",
        "    \n",
        "      \n",
        "    if epoch % show_interval == 0:\n",
        "      showImages(epoch, generator, rgb=True)\n",
        "      \n",
        "  return AutoObj(\n",
        "      gan=gan, \n",
        "      disc=discriminator,\n",
        "      gene=generator\n",
        "  )\n",
        "\n",
        "adam = Adam(lr=0.0002)\n",
        "def train_1D_working(epochs, batch_size, show_interval,  generator,   discriminator):\n",
        "  \n",
        "  X_train = getX_1D()\n",
        "  \n",
        "  batch_count = X_train.shape[0] / batch_size\n",
        "  \n",
        "  gan = gen_gan_net(discriminator, noise_dim, generator, adam)\n",
        "  \n",
        "  for epoch in range(1, epochs+1):\n",
        "    print(\"=\"*30 + \"Epoch \" + str(epoch) + \"=\"*30)\n",
        "    d_loss = g_loss = 0\n",
        "    \n",
        "    for indx in range(int(batch_count)):\n",
        "      noise = np.random.normal(0,1, size=[batch_size, noise_dim])\n",
        "      real_images = X_train[np.random.randint(0,X_train.shape[0], size=batch_size)] # get random images from X_train. len is batch_size\n",
        "      \n",
        "      fake_images = generator.predict(noise)\n",
        "      \n",
        "#       print(\"Fake shape: {}\\tReal shape:{}\".format(fake_images.shape,real_images.shape))\n",
        "      X = np.concatenate([real_images, fake_images])\n",
        "      \n",
        "      y_dis = np.concatenate([np.ones(batch_size), np.zeros(batch_size)])\n",
        "      \n",
        "      discriminator.trainable = True\n",
        "      d_loss += discriminator.train_on_batch(X, y_dis)[0]\n",
        "      \n",
        "      \n",
        "      \n",
        "#       # train the generator\n",
        "      noise = np.random.normal(0,1, size=[batch_size, noise_dim])\n",
        "      y_gen = np.ones(batch_size)\n",
        "#       print(\"noise shape: {}\\ty_gen shape: {}\".format(noise.shape, y_gen.shape))\n",
        "      discriminator.trainable = False\n",
        "      g_loss += gan.train_on_batch(noise, y_gen)[0]\n",
        "\n",
        "    print(\"g_loss={}\\td_loss={}\".format(g_loss/batch_count, d_loss/batch_count))\n",
        "    \n",
        "      \n",
        "    if epoch % show_interval == 0:\n",
        "      if channels!=1:\n",
        "        showImages(epoch, generator, rgb=True)\n",
        "      else:\n",
        "        showImages(epoch, generator)\n",
        "      \n",
        "  return AutoObj(\n",
        "      gan=gan, \n",
        "      disc=discriminator,\n",
        "      gene=generator\n",
        "  )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o7aIMhz3bRbl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### train_2D"
      ]
    },
    {
      "metadata": {
        "id": "QCVndryP_ShF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_2D(epochs, batch_size, show_interval, generator, discriminator):\n",
        "  \n",
        "  X_train = getX_2D()\n",
        "  \n",
        "  batch_count = X_train.shape[0] / batch_size\n",
        "\n",
        "  adam = gen_optimizer()\n",
        "\n",
        "  gan = gen_gan_net(discriminator, noise_dim, generator, adam)\n",
        "  \n",
        "  datagen = ImageDataGenerator(\n",
        "      rotation_range=7,\n",
        "      width_shift_range=0.05,\n",
        "      height_shift_range=0.05,\n",
        "      shear_range=0.01,\n",
        "      zoom_range=[0.9, 1.1],\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=False,      \n",
        "  )\n",
        "  # fit parameters from data\n",
        "  datagen.fit(X_train)\n",
        "  # configure batch size and retrieve one batch of images\n",
        "  epoch = 0\n",
        "  logs = []\n",
        "  for epoch in range(1, epochs+1):\n",
        "    g_loss = d_loss = 0\n",
        "    print(\"=\"*30 + \"Epoch \" + str(epoch) + \"=\"*30)\n",
        "    nbatch = 0\n",
        "    for X_batch in datagen.flow(X_train, batch_size=batch_size):\n",
        "      if nbatch >= batch_count:\n",
        "        break\n",
        "      nbatch += 1\n",
        "\n",
        "      if False: # Debug ImageDataGenerator\n",
        "        plt.figure()\n",
        "        for indx in range(batch_size):\n",
        "          img = X_batch[indx]\n",
        "          plt.subplot(5,6,indx+1)\n",
        "\n",
        "          plt.imshow(denormalize_rgb(img));\n",
        "        print(img[:10,:10])\n",
        "        return\n",
        "\n",
        "      n = X_batch.shape[0]\n",
        "      noise = np.random.normal(0,1, size=[n, noise_dim])\n",
        "      real_images = X_batch\n",
        "\n",
        "      fake_images = generator.predict(noise)\n",
        "    \n",
        "      \n",
        "      \n",
        "#       print(\"Fake shape: {}\\tReal shape:{}\".format(fake_images.shape,real_images.shape))\n",
        "      X = np.concatenate([real_images, fake_images])\n",
        "      y_dis = np.concatenate([np.ones(n), np.zeros(n)])\n",
        "      \n",
        "      discriminator.trainable = True\n",
        "#       print('X:{}\\ty_dis:{}'.format(str(X.shape), str(y_dis.shape)))\n",
        "      d_loss += discriminator.train_on_batch(X, y_dis)[0]\n",
        "      \n",
        "\n",
        "      \n",
        "      # train the generator\n",
        "      noise = np.random.normal(0,1, size=[n, noise_dim])\n",
        "      y_gen = np.ones(n)\n",
        "#       print(\"noise shape: {}\\ty_gen shape: {}\".format(noise.shape, y_gen.shape))\n",
        "      discriminator.trainable = False\n",
        "      g_loss += gan.train_on_batch(noise, y_gen)[0]\n",
        "    \n",
        "    print(\"g_loss={}\\td_loss={}\".format(str(g_loss/nbatch),str(d_loss/nbatch)))\n",
        "    \n",
        "    if epoch % show_interval == 0:\n",
        "      showImages(epoch, generator, rgb=True)\n",
        "      \n",
        "      \n",
        "  return AutoObj(\n",
        "      gan=gan, \n",
        "      disc=discriminator,\n",
        "      gene=generator\n",
        "  )\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W50y5dkWbXGA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run"
      ]
    },
    {
      "metadata": {
        "id": "jrZvq5pbb2GG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run 1D version"
      ]
    },
    {
      "metadata": {
        "id": "J1BFPRh-TWLx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GANI = train_1D_working(\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    show_interval=1,\n",
        "    generator = gen_generator_1D(Adam(0.0005)),\n",
        "    discriminator = gen_discriminator_1D_conv(\n",
        "        Adam(\n",
        "            lr=0.0002, \n",
        "            beta_1=0.5\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uIiQyA1xbszp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run 2D version\n",
        "\n",
        "#### Experimental results:\n",
        "##### Exp 1 - Good and stable gen-desc loss balance:\n",
        "- 50 epochs\n",
        "- 3000 images\n",
        "- Very stable DG matchup\n",
        "- Generated images are not very visually appealing\n",
        "\n",
        "code:\n",
        "\n",
        "\n",
        "```\n",
        "np.random.seed(16)\n",
        "GAN = train_2D(\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    show_interval=1,\n",
        "    generator = gen_generator_2D(\n",
        "        Adam(\n",
        "            lr=0.02,\n",
        "            beta_1=0.5\n",
        "        )\n",
        "    ),\n",
        "    discriminator = gen_discriminator_2D_conv(\n",
        "        Adam(\n",
        "            lr=0.00004, \n",
        "            beta_1=0.5\n",
        "        )\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JIjM4MksxL78",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(16)\n",
        "GAN = train_2D(\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    show_interval=1,\n",
        "    generator = gen_generator_2D(\n",
        "        Adam(\n",
        "            lr=0.02,\n",
        "            beta_1=0.5\n",
        "        )\n",
        "    ),\n",
        "    discriminator = gen_discriminator_2D_conv(\n",
        "        Adam(\n",
        "            lr=0.00004, \n",
        "            beta_1=0.5\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "80NqxHEQ7UeV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for indx in range(30):\n",
        "  plt.subplot(5,6,indx+1)\n",
        "  plt.imshow(data[indx])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}